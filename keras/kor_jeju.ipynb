{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 587
    },
    "colab_type": "code",
    "id": "Pa4IReo6Cc7a",
    "outputId": "686f61e0-9abf-499a-c8d6-73dd43f1dbed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 3876883829019089860, name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 5933003659533636849\n",
       " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 12620358854082576940\n",
       " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 15956161332\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 11978132078730237417\n",
       " physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "device_lib.list_local_devices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iNlo6VGXFllH"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-xGdGylMFawA",
    "outputId": "538ebef1-6ecc-4e12-95c3-03450cfe6900"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 466\n",
      "Number of unique input tokens: 498\n",
      "Number of unique output tokens: 482\n",
      "Max sequence length for inputs: 37\n",
      "Max sequence length for outputs: 41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[False, False,  True, ...,  True,  True,  True],\n",
       "        [ True, False,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       [[False, False,  True, ...,  True,  True,  True],\n",
       "        [ True, False,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       [[False,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[False,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True, False,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       [[False,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True, False,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       [[False,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True, False,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'jeju_final.txt'\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    target_text = '\\t' + target_text\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "            \n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1. # 원핫인코딩해준거임\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "b_Q2fuUMCOaO",
    "outputId": "4a857a78-c545-42f3-8846-1d103b92f838"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 466\n",
      "Number of unique input tokens: 498\n",
      "Number of unique output tokens: 483\n",
      "Max sequence length for inputs: 37\n",
      "Max sequence length for outputs: 42\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_13\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_13\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_13\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_13\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_13\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_13\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_13\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 372 samples, validate on 94 samples\n",
      "Epoch 1/100\n",
      "372/372 [==============================] - 6s 16ms/step - loss: 3.3746 - acc: 0.7117 - val_loss: 2.1583 - val_acc: 0.7112\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.15826, saving model to keras_model1.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ICT01_13\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.9528 - acc: 0.8604 - val_loss: 2.0681 - val_acc: 0.7112\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.15826 to 2.06805, saving model to keras_model1.model\n",
      "Epoch 3/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.9213 - acc: 0.8604 - val_loss: 2.0497 - val_acc: 0.7112\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.06805 to 2.04969, saving model to keras_model1.model\n",
      "Epoch 4/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.8968 - acc: 0.8604 - val_loss: 2.0473 - val_acc: 0.7112\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.04969 to 2.04733, saving model to keras_model1.model\n",
      "Epoch 5/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.8715 - acc: 0.8604 - val_loss: 2.0375 - val_acc: 0.7112\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.04733 to 2.03753, saving model to keras_model1.model\n",
      "Epoch 6/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.8766 - acc: 0.8621 - val_loss: 2.1847 - val_acc: 0.7064\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.03753\n",
      "Epoch 7/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.8496 - acc: 0.8616 - val_loss: 2.3121 - val_acc: 0.7112\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.03753\n",
      "Epoch 8/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.8536 - acc: 0.8598 - val_loss: 2.2594 - val_acc: 0.7024\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.03753\n",
      "Epoch 9/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.8999 - acc: 0.8570 - val_loss: 2.1628 - val_acc: 0.6993\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.03753\n",
      "Epoch 10/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.8355 - acc: 0.8640 - val_loss: 2.1677 - val_acc: 0.6925\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.03753\n",
      "Epoch 11/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.8211 - acc: 0.8630 - val_loss: 2.0635 - val_acc: 0.6915\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.03753\n",
      "Epoch 12/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.8223 - acc: 0.8634 - val_loss: 2.0569 - val_acc: 0.6958\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.03753\n",
      "Epoch 13/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.8244 - acc: 0.8615 - val_loss: 2.1099 - val_acc: 0.6920\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.03753\n",
      "Epoch 14/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.8125 - acc: 0.8633 - val_loss: 2.1666 - val_acc: 0.6943\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.03753\n",
      "Epoch 15/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.8528 - acc: 0.8637 - val_loss: 2.2966 - val_acc: 0.6976\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.03753\n",
      "Epoch 16/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.7936 - acc: 0.8651 - val_loss: 2.2687 - val_acc: 0.6930\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 2.03753\n",
      "Epoch 17/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.7899 - acc: 0.8649 - val_loss: 2.1163 - val_acc: 0.6933\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.03753\n",
      "Epoch 18/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.7883 - acc: 0.8651 - val_loss: 2.2374 - val_acc: 0.6907\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.03753\n",
      "Epoch 19/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.7963 - acc: 0.8658 - val_loss: 2.3378 - val_acc: 0.6933\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.03753\n",
      "Epoch 20/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.7767 - acc: 0.8646 - val_loss: 2.3275 - val_acc: 0.6943\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.03753\n",
      "Epoch 21/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.7665 - acc: 0.8655 - val_loss: 2.3418 - val_acc: 0.6948\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.03753\n",
      "Epoch 22/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.8076 - acc: 0.8658 - val_loss: 2.4486 - val_acc: 0.6940\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2.03753\n",
      "Epoch 23/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.7579 - acc: 0.8679 - val_loss: 2.2473 - val_acc: 0.6907\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2.03753\n",
      "Epoch 24/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.7584 - acc: 0.8684 - val_loss: 2.2128 - val_acc: 0.6915\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2.03753\n",
      "Epoch 25/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.7705 - acc: 0.8665 - val_loss: 2.3343 - val_acc: 0.6925\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2.03753\n",
      "Epoch 26/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.7314 - acc: 0.8702 - val_loss: 2.5185 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.03753\n",
      "Epoch 27/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.7364 - acc: 0.8700 - val_loss: 2.4051 - val_acc: 0.6884\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.03753\n",
      "Epoch 28/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.7607 - acc: 0.8709 - val_loss: 2.4765 - val_acc: 0.6973\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2.03753\n",
      "Epoch 29/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.7145 - acc: 0.8715 - val_loss: 2.3937 - val_acc: 0.6890\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.03753\n",
      "Epoch 30/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.7380 - acc: 0.8718 - val_loss: 2.8706 - val_acc: 0.6882\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.03753\n",
      "Epoch 31/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.7128 - acc: 0.8708 - val_loss: 2.4945 - val_acc: 0.6897\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 2.03753\n",
      "Epoch 32/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.7276 - acc: 0.8717 - val_loss: 2.7127 - val_acc: 0.6973\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 2.03753\n",
      "Epoch 33/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.6925 - acc: 0.8755 - val_loss: 2.4909 - val_acc: 0.6953\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 2.03753\n",
      "Epoch 34/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.6987 - acc: 0.8746 - val_loss: 2.4445 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 2.03753\n",
      "Epoch 35/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.6782 - acc: 0.8764 - val_loss: 2.5589 - val_acc: 0.6960\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 2.03753\n",
      "Epoch 36/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.6958 - acc: 0.8746 - val_loss: 2.3419 - val_acc: 0.7019\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 2.03753\n",
      "Epoch 37/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.7224 - acc: 0.8719 - val_loss: 2.7387 - val_acc: 0.6928\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 2.03753\n",
      "Epoch 38/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.6715 - acc: 0.8769 - val_loss: 2.3722 - val_acc: 0.7006\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 2.03753\n",
      "Epoch 39/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.6495 - acc: 0.8787 - val_loss: 2.6820 - val_acc: 0.6958\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 2.03753\n",
      "Epoch 40/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.6505 - acc: 0.8769 - val_loss: 2.3070 - val_acc: 0.6887\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 2.03753\n",
      "Epoch 41/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.6982 - acc: 0.8788 - val_loss: 2.2870 - val_acc: 0.7006\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 2.03753\n",
      "Epoch 42/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.6369 - acc: 0.8787 - val_loss: 2.4107 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 2.03753\n",
      "Epoch 43/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.6356 - acc: 0.8797 - val_loss: 2.6774 - val_acc: 0.6993\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 2.03753\n",
      "Epoch 44/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.6284 - acc: 0.8797 - val_loss: 2.4770 - val_acc: 0.6933\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 2.03753\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372/372 [==============================] - 1s 2ms/step - loss: 0.6393 - acc: 0.8774 - val_loss: 2.8008 - val_acc: 0.7044\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 2.03753\n",
      "Epoch 46/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.7434 - acc: 0.8754 - val_loss: 2.4309 - val_acc: 0.6935\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 2.03753\n",
      "Epoch 47/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.6148 - acc: 0.8810 - val_loss: 2.3763 - val_acc: 0.6978\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 2.03753\n",
      "Epoch 48/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.6034 - acc: 0.8829 - val_loss: 2.5361 - val_acc: 0.6955\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 2.03753\n",
      "Epoch 49/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.6112 - acc: 0.8802 - val_loss: 2.6806 - val_acc: 0.6991\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 2.03753\n",
      "Epoch 50/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.6251 - acc: 0.8820 - val_loss: 2.5784 - val_acc: 0.6897\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 2.03753\n",
      "Epoch 51/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.5870 - acc: 0.8830 - val_loss: 2.5055 - val_acc: 0.6928\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 2.03753\n",
      "Epoch 52/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.5867 - acc: 0.8838 - val_loss: 2.5347 - val_acc: 0.6943\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 2.03753\n",
      "Epoch 53/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.6299 - acc: 0.8840 - val_loss: 2.4306 - val_acc: 0.6492\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 2.03753\n",
      "Epoch 54/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.6029 - acc: 0.8813 - val_loss: 2.4618 - val_acc: 0.6971\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 2.03753\n",
      "Epoch 55/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.5688 - acc: 0.8851 - val_loss: 2.4635 - val_acc: 0.7016\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 2.03753\n",
      "Epoch 56/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.5649 - acc: 0.8858 - val_loss: 2.4911 - val_acc: 0.6928\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 2.03753\n",
      "Epoch 57/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.5617 - acc: 0.8852 - val_loss: 2.6040 - val_acc: 0.6753\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 2.03753\n",
      "Epoch 58/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.5871 - acc: 0.8840 - val_loss: 2.6308 - val_acc: 0.6821\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 2.03753\n",
      "Epoch 59/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.5505 - acc: 0.8865 - val_loss: 2.5449 - val_acc: 0.6841\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 2.03753\n",
      "Epoch 60/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.5505 - acc: 0.8865 - val_loss: 2.7362 - val_acc: 0.6933\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 2.03753\n",
      "Epoch 61/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.5448 - acc: 0.8886 - val_loss: 2.4193 - val_acc: 0.7006\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 2.03753\n",
      "Epoch 62/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.5460 - acc: 0.8885 - val_loss: 2.6035 - val_acc: 0.6943\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 2.03753\n",
      "Epoch 63/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.5309 - acc: 0.8895 - val_loss: 2.5009 - val_acc: 0.6973\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 2.03753\n",
      "Epoch 64/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.5406 - acc: 0.8897 - val_loss: 2.2635 - val_acc: 0.6917\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 2.03753\n",
      "Epoch 65/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.5281 - acc: 0.8912 - val_loss: 2.3988 - val_acc: 0.6930\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 2.03753\n",
      "Epoch 66/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.5245 - acc: 0.8918 - val_loss: 2.3862 - val_acc: 0.7036\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 2.03753\n",
      "Epoch 67/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.5197 - acc: 0.8918 - val_loss: 2.4122 - val_acc: 0.7085\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 2.03753\n",
      "Epoch 68/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.5033 - acc: 0.8927 - val_loss: 2.5312 - val_acc: 0.7107\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 2.03753\n",
      "Epoch 69/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.4991 - acc: 0.8930 - val_loss: 2.5264 - val_acc: 0.7082\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 2.03753\n",
      "Epoch 70/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.5086 - acc: 0.8930 - val_loss: 2.3425 - val_acc: 0.6801\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 2.03753\n",
      "Epoch 71/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.5118 - acc: 0.8957 - val_loss: 2.3397 - val_acc: 0.7031\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 2.03753\n",
      "Epoch 72/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.4829 - acc: 0.8982 - val_loss: 2.4031 - val_acc: 0.7044\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 2.03753\n",
      "Epoch 73/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.4736 - acc: 0.8987 - val_loss: 2.3913 - val_acc: 0.7044\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 2.03753\n",
      "Epoch 74/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.4723 - acc: 0.9003 - val_loss: 2.3826 - val_acc: 0.7105\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 2.03753\n",
      "Epoch 75/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.4800 - acc: 0.8985 - val_loss: 2.3596 - val_acc: 0.7036\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 2.03753\n",
      "Epoch 76/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.4634 - acc: 0.9026 - val_loss: 2.4437 - val_acc: 0.7077\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 2.03753\n",
      "Epoch 77/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.4569 - acc: 0.9030 - val_loss: 2.2994 - val_acc: 0.6852\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 2.03753\n",
      "Epoch 78/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.4652 - acc: 0.9040 - val_loss: 2.4990 - val_acc: 0.7047\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 2.03753\n",
      "Epoch 79/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.4555 - acc: 0.9042 - val_loss: 2.6353 - val_acc: 0.6976\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 2.03753\n",
      "Epoch 80/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.4442 - acc: 0.9073 - val_loss: 2.2737 - val_acc: 0.7080\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 2.03753\n",
      "Epoch 81/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.4288 - acc: 0.9089 - val_loss: 2.4108 - val_acc: 0.7054\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 2.03753\n",
      "Epoch 82/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.4429 - acc: 0.9060 - val_loss: 2.3281 - val_acc: 0.6991\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 2.03753\n",
      "Epoch 83/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.4248 - acc: 0.9099 - val_loss: 2.2676 - val_acc: 0.6968\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 2.03753\n",
      "Epoch 84/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.4271 - acc: 0.9106 - val_loss: 2.3354 - val_acc: 0.7042\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 2.03753\n",
      "Epoch 85/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.4107 - acc: 0.9137 - val_loss: 2.4402 - val_acc: 0.7100\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 2.03753\n",
      "Epoch 86/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.4153 - acc: 0.9122 - val_loss: 2.2975 - val_acc: 0.6998\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 2.03753\n",
      "Epoch 87/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.4024 - acc: 0.9149 - val_loss: 2.3599 - val_acc: 0.7064\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 2.03753\n",
      "Epoch 88/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.3970 - acc: 0.9144 - val_loss: 2.3257 - val_acc: 0.6915\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 2.03753\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372/372 [==============================] - 1s 2ms/step - loss: 0.3950 - acc: 0.9163 - val_loss: 2.4671 - val_acc: 0.7026\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 2.03753\n",
      "Epoch 90/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.3828 - acc: 0.9190 - val_loss: 2.3215 - val_acc: 0.7034\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 2.03753\n",
      "Epoch 91/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.3907 - acc: 0.9165 - val_loss: 2.3063 - val_acc: 0.7006\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 2.03753\n",
      "Epoch 92/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.3666 - acc: 0.9206 - val_loss: 2.4730 - val_acc: 0.7039\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 2.03753\n",
      "Epoch 93/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.3792 - acc: 0.9180 - val_loss: 2.3454 - val_acc: 0.7118\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 2.03753\n",
      "Epoch 94/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.3597 - acc: 0.9231 - val_loss: 2.3366 - val_acc: 0.7014\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 2.03753\n",
      "Epoch 95/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.3611 - acc: 0.9224 - val_loss: 2.3376 - val_acc: 0.7011\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 2.03753\n",
      "Epoch 96/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.3493 - acc: 0.9234 - val_loss: 2.4338 - val_acc: 0.7044\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 2.03753\n",
      "Epoch 97/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.3525 - acc: 0.9238 - val_loss: 2.3754 - val_acc: 0.6976\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 2.03753\n",
      "Epoch 98/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.3452 - acc: 0.9256 - val_loss: 2.4975 - val_acc: 0.7059\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 2.03753\n",
      "Epoch 99/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.3322 - acc: 0.9288 - val_loss: 2.4025 - val_acc: 0.7006\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 2.03753\n",
      "Epoch 100/100\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.3307 - acc: 0.9283 - val_loss: 2.4231 - val_acc: 0.6907\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 2.03753\n",
      "-\n",
      "Input sentence: 하르방 \n",
      "Decoded sentence:  감물들인옷\n",
      "\n",
      "-\n",
      "Input sentence: 할망 \n",
      "Decoded sentence:  감물들인옷\n",
      "\n",
      "-\n",
      "Input sentence: 아방 \n",
      "Decoded sentence:  바지\n",
      "\n",
      "-\n",
      "Input sentence: 어멍 \n",
      "Decoded sentence:  감물들인옷\n",
      "\n",
      "-\n",
      "Input sentence: 비바리 \n",
      "Decoded sentence:  돼지\n",
      "\n",
      "-\n",
      "Input sentence: 괸당 \n",
      "Decoded sentence:  돼지\n",
      "\n",
      "-\n",
      "Input sentence: 걸바시 \n",
      "Decoded sentence:  여자\n",
      "\n",
      "-\n",
      "Input sentence: 넹바리 \n",
      "Decoded sentence:  감물들인옷\n",
      "\n",
      "-\n",
      "Input sentence: 다슴아돌 \n",
      "Decoded sentence:  감물들인옷\n",
      "\n",
      "-\n",
      "Input sentence: 말젯놈 \n",
      "Decoded sentence:  감물들인 옷\n",
      "\n",
      "-\n",
      "Input sentence: 소나이 \n",
      "Decoded sentence:  돼지\n",
      "\n",
      "-\n",
      "Input sentence: 성님 \n",
      "Decoded sentence:  소\n",
      "\n",
      "-\n",
      "Input sentence: 작산 거 \n",
      "Decoded sentence:  감물들인옷\n",
      "\n",
      "-\n",
      "Input sentence: 좀녀 \n",
      "Decoded sentence:  여자\n",
      "\n",
      "-\n",
      "Input sentence: 촐람생이 \n",
      "Decoded sentence:  감물들인옷\n",
      "\n",
      "-\n",
      "Input sentence: 홀아방 \n",
      "Decoded sentence:  상복\n",
      "\n",
      "-\n",
      "Input sentence: 가달 \n",
      "Decoded sentence:  돼지\n",
      "\n",
      "-\n",
      "Input sentence: 꼴랑지 \n",
      "Decoded sentence:  감물들인옷\n",
      "\n",
      "-\n",
      "Input sentence: 구뚱배기 \n",
      "Decoded sentence:  머리\n",
      "\n",
      "-\n",
      "Input sentence: 꽝 \n",
      "Decoded sentence:  돼지\n",
      "\n",
      "-\n",
      "Input sentence: 굴레 \n",
      "Decoded sentence:  소\n",
      "\n",
      "-\n",
      "Input sentence: 대망생이 \n",
      "Decoded sentence:  머리\n",
      "\n",
      "-\n",
      "Input sentence: 등땡이 \n",
      "Decoded sentence:  바지\n",
      "\n",
      "-\n",
      "Input sentence: 또꼬망 \n",
      "Decoded sentence:  바지\n",
      "\n",
      "-\n",
      "Input sentence: 모감지 \n",
      "Decoded sentence:  소\n",
      "\n",
      "-\n",
      "Input sentence: 베 봉탱이 \n",
      "Decoded sentence:  감물들인인옷\n",
      "\n",
      "-\n",
      "Input sentence: 베아지 볼라불라\n",
      "Decoded sentence:  감물들인인옷\n",
      "\n",
      "-\n",
      "Input sentence: 상판이 \n",
      "Decoded sentence:  돼지\n",
      "\n",
      "-\n",
      "Input sentence: 야게기 \n",
      "Decoded sentence:  머리\n",
      "\n",
      "-\n",
      "Input sentence: 야굴탁 \n",
      "Decoded sentence:  머리\n",
      "\n",
      "-\n",
      "Input sentence: 임댕이 \n",
      "Decoded sentence:  머리\n",
      "\n",
      "-\n",
      "Input sentence: 정겡이 \n",
      "Decoded sentence:  상복\n",
      "\n",
      "-\n",
      "Input sentence: 저껭이 \n",
      "Decoded sentence:  여자\n",
      "\n",
      "-\n",
      "Input sentence: 조금태기 \n",
      "Decoded sentence: 지\n",
      "\n",
      "-\n",
      "Input sentence: 좀짐팽이 \n",
      "Decoded sentence:  상복\n",
      "\n",
      "-\n",
      "Input sentence: 허운데기 \n",
      "Decoded sentence:                                            \n",
      "-\n",
      "Input sentence: 허벅다리 \n",
      "Decoded sentence:  감물들인인옷\n",
      "\n",
      "-\n",
      "Input sentence: 놋 \n",
      "Decoded sentence:  돼지\n",
      "\n",
      "-\n",
      "Input sentence: 간수메 \n",
      "Decoded sentence:  감물들인 옷\n",
      "\n",
      "-\n",
      "Input sentence: 개역 \n",
      "Decoded sentence:  감물들인 옷\n",
      "\n",
      "-\n",
      "Input sentence: 것 \n",
      "Decoded sentence:  감물들인옷\n",
      "\n",
      "-\n",
      "Input sentence: 괴기 \n",
      "Decoded sentence:  상복\n",
      "\n",
      "-\n",
      "Input sentence: 바당괴기 \n",
      "Decoded sentence:  소\n",
      "\n",
      "-\n",
      "Input sentence: 돗괴기 \n",
      "Decoded sentence:  소\n",
      "\n",
      "-\n",
      "Input sentence: 쇠괴기 \n",
      "Decoded sentence:  머리\n",
      "\n",
      "-\n",
      "Input sentence: 도괴기 \n",
      "Decoded sentence:  소\n",
      "\n",
      "-\n",
      "Input sentence: 곤떡 \n",
      "Decoded sentence:  감물들인옷\n",
      "\n",
      "-\n",
      "Input sentence: 곤밥 \n",
      "Decoded sentence:  감물들인 옷\n",
      "\n",
      "-\n",
      "Input sentence: 놈삐 \n",
      "Decoded sentence:  감물들인옷\n",
      "\n",
      "-\n",
      "Input sentence: 대사니김치 \n",
      "Decoded sentence:  바지\n",
      "\n",
      "-\n",
      "Input sentence: 마농 \n",
      "Decoded sentence:  돼지\n",
      "\n",
      "-\n",
      "Input sentence: 마농 \n",
      "Decoded sentence:  돼지\n",
      "\n",
      "-\n",
      "Input sentence: 조배기 \n",
      "Decoded sentence:  감물들인 옷\n",
      "\n",
      "-\n",
      "Input sentence: 촐래 \n",
      "Decoded sentence:  여자\n",
      "\n",
      "-\n",
      "Input sentence: 촘지금 \n",
      "Decoded sentence:  감물들인옷\n",
      "\n",
      "-\n",
      "Input sentence: 짐치 \n",
      "Decoded sentence:  상복\n",
      "\n",
      "-\n",
      "Input sentence: 촙쏠 \n",
      "Decoded sentence:  돼지\n",
      "\n",
      "-\n",
      "Input sentence: 조팝 \n",
      "Decoded sentence:  바지\n",
      "\n",
      "-\n",
      "Input sentence: 갈옷 \n",
      "Decoded sentence:  감물들인옷\n",
      "\n",
      "-\n",
      "Input sentence: 갈 적삼 \n",
      "Decoded sentence:  감물들인옷\n",
      "\n",
      "-\n",
      "Input sentence: 갈 중이 \n",
      "Decoded sentence:  감물들인옷\n",
      "\n",
      "-\n",
      "Input sentence: 강알터진 바지 \n",
      "Decoded sentence:  감물들인옷\n",
      "\n",
      "-\n",
      "Input sentence: 게와 \n",
      "Decoded sentence:  여자\n",
      "\n",
      "-\n",
      "Input sentence: 단취 \n",
      "Decoded sentence:  머리\n",
      "\n",
      "-\n",
      "Input sentence: 밀랑 페랭이 \n",
      "Decoded sentence:  여자\n",
      "\n",
      "-\n",
      "Input sentence: 보선 \n",
      "Decoded sentence:  여자\n",
      "\n",
      "-\n",
      "Input sentence: 소중이 \n",
      "Decoded sentence:  소\n",
      "\n",
      "-\n",
      "Input sentence: 신착 \n",
      "Decoded sentence:  감물들인 옷\n",
      "\n",
      "-\n",
      "Input sentence: 찍신 \n",
      "Decoded sentence:  소\n",
      "\n",
      "-\n",
      "Input sentence: 좀뱅이 \n",
      "Decoded sentence:  여자\n",
      "\n",
      "-\n",
      "Input sentence: 등지게 \n",
      "Decoded sentence:  감물들인옷\n",
      "\n",
      "-\n",
      "Input sentence: 고장중이 \n",
      "Decoded sentence:  머리\n",
      "\n",
      "-\n",
      "Input sentence: 도폭 \n",
      "Decoded sentence:  감물들인옷\n",
      "\n",
      "-\n",
      "Input sentence: 두루막 \n",
      "Decoded sentence:  소\n",
      "\n",
      "-\n",
      "Input sentence: 베불레기 \n",
      "Decoded sentence:  머리\n",
      "\n",
      "-\n",
      "Input sentence: 우장 \n",
      "Decoded sentence:  바지\n",
      "\n",
      "-\n",
      "Input sentence: 저구리 \n",
      "Decoded sentence:  돼지\n",
      "\n",
      "-\n",
      "Input sentence: 지성귀 \n",
      "Decoded sentence:  돼지\n",
      "\n",
      "-\n",
      "Input sentence: 지서귀 \n",
      "Decoded sentence:  소\n",
      "\n",
      "-\n",
      "Input sentence: 쪼께 \n",
      "Decoded sentence:  돼지\n",
      "\n",
      "-\n",
      "Input sentence: 치메 \n",
      "Decoded sentence:  상복\n",
      "\n",
      "-\n",
      "Input sentence: 건대 \n",
      "Decoded sentence:  감물들인 옷\n",
      "\n",
      "-\n",
      "Input sentence: 사모관대 \n",
      "Decoded sentence:  감물들인옷\n",
      "\n",
      "-\n",
      "Input sentence: 시미옷 \n",
      "Decoded sentence:  여자\n",
      "\n",
      "-\n",
      "Input sentence: 제복 \n",
      "Decoded sentence:  소\n",
      "\n",
      "-\n",
      "Input sentence: 망근 \n",
      "Decoded sentence:  돼지\n",
      "\n",
      "-\n",
      "Input sentence: 방립 \n",
      "Decoded sentence:  바지\n",
      "\n",
      "-\n",
      "Input sentence: 벙것 \n",
      "Decoded sentence:  여자\n",
      "\n",
      "-\n",
      "Input sentence: 상갓 \n",
      "Decoded sentence:  바지\n",
      "\n",
      "-\n",
      "Input sentence: 탕근 \n",
      "Decoded sentence:  여자\n",
      "\n",
      "-\n",
      "Input sentence: 풍뎅이 \n",
      "Decoded sentence:  돼지\n",
      "\n",
      "-\n",
      "Input sentence: 휘양 \n",
      "Decoded sentence:  돼지\n",
      "\n",
      "-\n",
      "Input sentence: 낭저 \n",
      "Decoded sentence:  여자\n",
      "\n",
      "-\n",
      "Input sentence: 달리 \n",
      "Decoded sentence:  바지\n",
      "\n",
      "-\n",
      "Input sentence: 빈네 \n",
      "Decoded sentence:  돼지\n",
      "\n",
      "-\n",
      "Input sentence: 상퉁이 \n",
      "Decoded sentence:  소\n",
      "\n",
      "-\n",
      "Input sentence: 얼레기 \n",
      "Decoded sentence:  머리\n",
      "\n",
      "-\n",
      "Input sentence: 얼레빗 \n",
      "Decoded sentence:  머리\n",
      "\n",
      "-\n",
      "Input sentence: 쪽도리 \n",
      "Decoded sentence:  돼지\n",
      "\n",
      "-\n",
      "Input sentence: 쳉빗 \n",
      "Decoded sentence:  머리\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 500  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'jeju_final.txt'\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text: # 단어가 아니라 글자단위로 잘라서 문제가 됨\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep (왜?????????????)\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens)) #  텐서를 반환함\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "che = 'keras_model1.model'\n",
    "point = ModelCheckpoint(filepath=che , monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2,callbacks=[point,early_stopping])\n",
    "# Save model\n",
    "model.save('s2s.h5')\n",
    "\n",
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 37, 498)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[1:2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(466, 37, 498)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "kor_jeju.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
